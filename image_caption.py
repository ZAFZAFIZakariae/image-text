"""Image captioning helpers for BLIP and InstructBLIP checkpoints.

This module provides a :func:`caption_image` function that loads either a local
image or one referenced by a URL and generates a caption using a BLIP-family
model hosted on Hugging Face.  Both the classic BLIP captioning checkpoints and
the newer InstructBLIP variants are supported so callers can pick the captioning
style that best fits their workflow.
"""

from __future__ import annotations

from io import BytesIO
from typing import Any, Dict, Tuple
from urllib.parse import urlparse

import requests
import torch
from PIL import Image
from transformers import (
    BlipForConditionalGeneration,
    BlipProcessor,
    InstructBlipForConditionalGeneration,
    InstructBlipProcessor,
)

# Global cache of loaded processors/models keyed by model identifier so the
# weights are only materialized once per checkpoint. Loading these models is
# relatively expensive, so keeping the objects around avoids paying the cost for
# every call to ``caption_image``.  The final string in the tuple indicates which
# processor/model pair was created so the captioning logic can branch
# appropriately.
_LOADED_MODELS: Dict[str, Tuple[Any, Any, str]] = {}
_DEVICE: torch.device | None = None


def _is_url(path_or_url: str) -> bool:
    """Return ``True`` when ``path_or_url`` looks like an HTTP(S) URL."""

    parsed = urlparse(path_or_url)
    return parsed.scheme in {"http", "https"} and bool(parsed.netloc)


def _load_model(
    model_name: str = "Salesforce/instructblip-flan-t5-xl",
) -> Tuple[Any, Any, torch.device, str]:
    """Lazy-load the processor and model, returning both along with the device.

    The function caches the loaded objects in module-level globals to ensure that
    multiple caption requests reuse the same processor and model. The cache is
    keyed by ``model_name`` so callers can select different BLIP or InstructBLIP
    checkpoints.
    """

    global _DEVICE

    if _DEVICE is None:
        # Choose GPU when available, otherwise fall back to CPU execution.
        _DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    if model_name not in _LOADED_MODELS:
        model_name_lower = model_name.lower()

        if "instructblip" in model_name_lower:
            processor = InstructBlipProcessor.from_pretrained(model_name)
            model = (
                InstructBlipForConditionalGeneration.from_pretrained(model_name)
                .to(_DEVICE)
            )
            architecture = "instructblip"
        else:
            processor = BlipProcessor.from_pretrained(model_name)
            model = BlipForConditionalGeneration.from_pretrained(model_name).to(
                _DEVICE
            )
            architecture = "blip"

        # Ensure the model is in evaluation mode. Caption generation does not
        # require gradient computation, so we disable it to save memory.
        model.eval()
        _LOADED_MODELS[model_name] = (processor, model, architecture)

    processor, model, architecture = _LOADED_MODELS[model_name]
    assert _DEVICE is not None
    return processor, model, _DEVICE, architecture


def caption_image(
    image_path_or_url: str,
    *,
    model_name: str = "Salesforce/instructblip-flan-t5-xl",
    instruct_prompt: str = "Describe the image in detail.",
    max_new_tokens: int = 128,
    num_beams: int = 1,
    do_sample: bool = False,
    temperature: float | None = None,
) -> str:
    """Generate a descriptive caption for ``image_path_or_url``.

    Args:
        image_path_or_url: Either a filesystem path to an image or an HTTP(S)
            URL that returns image bytes.
        model_name: Hugging Face checkpoint to use for captioning.
        instruct_prompt: Prompt forwarded to InstructBLIP checkpoints. Ignored for
            classic BLIP models.
        max_new_tokens: Upper bound on the number of tokens produced during
            generation.
        num_beams: Beam width to use when decoding.
        do_sample: When ``True``, enables sampling instead of greedy decoding.
        temperature: Optional sampling temperature to apply when ``do_sample`` is
            enabled.

    Returns:
        A caption generated by the BLIP model.

    Raises:
        requests.HTTPError: If downloading an image from a URL fails.
        PIL.UnidentifiedImageError: If the data cannot be interpreted as an image.
    """

    processor, model, device, architecture = _load_model(model_name)

    # Load the image from disk or over HTTP. We convert the image to RGB because
    # BLIP expects three channels.
    if _is_url(image_path_or_url):
        response = requests.get(image_path_or_url, timeout=30)
        response.raise_for_status()
        image = Image.open(BytesIO(response.content)).convert("RGB")
    else:
        image = Image.open(image_path_or_url).convert("RGB")

    # Run the image through the processor and the model to obtain caption tokens.
    if architecture == "instructblip":
        inputs = processor(
            images=image, text=instruct_prompt, return_tensors="pt"
        ).to(device)
    else:
        inputs = processor(images=image, return_tensors="pt").to(device)

    generation_kwargs: Dict[str, Any] = {
        "max_new_tokens": max_new_tokens,
        "num_beams": num_beams,
        "do_sample": do_sample,
    }
    if temperature is not None:
        generation_kwargs["temperature"] = temperature

    with torch.no_grad():
        output = model.generate(**inputs, **generation_kwargs)

    # Decode the generated tokens back into text and strip excess whitespace.
    if architecture == "instructblip":
        caption = processor.batch_decode(output, skip_special_tokens=True)[0].strip()
    else:
        caption = processor.decode(output[0], skip_special_tokens=True).strip()
    return caption


if __name__ == "__main__":
    # A simple manual test that fetches a demo image and prints the caption.
    SAMPLE_IMAGE_URL = "https://huggingface.co/datasets/Narsil/image_dummy/resolve/main/parrots.png"

    print("Generating caption for sample image...")
    generated_caption = caption_image(SAMPLE_IMAGE_URL)
    print(f"Caption: {generated_caption}")
