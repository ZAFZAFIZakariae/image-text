"""Image captioning module using the BLIP base model.

This module provides a :func:`caption_image` function that loads either a local
image or one referenced by a URL and generates a caption using the
Salesforce BLIP image captioning model. The code follows the
recommendations from the BLIP model card on Hugging Face.
"""

from __future__ import annotations

from io import BytesIO
from typing import Tuple
from urllib.parse import urlparse

import requests
import torch
from PIL import Image
from transformers import BlipForConditionalGeneration, BlipProcessor

# Global singleton references to the processor and model so that they are only
# loaded once. Loading the BLIP model is relatively expensive, so keeping the
# objects around avoids paying the cost for every call to ``caption_image``.
_PROCESSOR: BlipProcessor | None = None
_MODEL: BlipForConditionalGeneration | None = None
_DEVICE: torch.device | None = None


def _is_url(path_or_url: str) -> bool:
    """Return ``True`` when ``path_or_url`` looks like an HTTP(S) URL."""

    parsed = urlparse(path_or_url)
    return parsed.scheme in {"http", "https"} and bool(parsed.netloc)


def _load_model() -> Tuple[BlipProcessor, BlipForConditionalGeneration, torch.device]:
    """Lazy-load the BLIP processor and model, returning both along with the device.

    The function caches the loaded objects in module-level globals to ensure that
    multiple caption requests reuse the same processor and model.
    """

    global _PROCESSOR, _MODEL, _DEVICE

    if _PROCESSOR is None or _MODEL is None or _DEVICE is None:
        # Choose GPU when available, otherwise fall back to CPU execution.
        _DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

        # Load the pretrained processor and model as recommended in the BLIP
        # documentation. The "Salesforce/blip-image-captioning-base" checkpoint
        # is a good default for general-purpose captioning.
        _PROCESSOR = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base")
        _MODEL = BlipForConditionalGeneration.from_pretrained(
            "Salesforce/blip-image-captioning-base"
        ).to(_DEVICE)

        # Ensure the model is in evaluation mode. Caption generation does not
        # require gradient computation, so we disable it to save memory.
        _MODEL.eval()

    # The type checker needs reassurance that the globals are populated.
    assert _PROCESSOR is not None
    assert _MODEL is not None
    assert _DEVICE is not None
    return _PROCESSOR, _MODEL, _DEVICE


def caption_image(image_path_or_url: str) -> str:
    """Generate a descriptive caption for ``image_path_or_url``.

    Args:
        image_path_or_url: Either a filesystem path to an image or an HTTP(S)
            URL that returns image bytes.

    Returns:
        A caption generated by the BLIP model.

    Raises:
        requests.HTTPError: If downloading an image from a URL fails.
        PIL.UnidentifiedImageError: If the data cannot be interpreted as an image.
    """

    processor, model, device = _load_model()

    # Load the image from disk or over HTTP. We convert the image to RGB because
    # BLIP expects three channels.
    if _is_url(image_path_or_url):
        response = requests.get(image_path_or_url, timeout=30)
        response.raise_for_status()
        image = Image.open(BytesIO(response.content)).convert("RGB")
    else:
        image = Image.open(image_path_or_url).convert("RGB")

    # Run the image through the processor and the model to obtain caption tokens.
    inputs = processor(images=image, return_tensors="pt").to(device)

    with torch.no_grad():
        output = model.generate(**inputs)

    # Decode the generated tokens back into text and strip excess whitespace.
    caption = processor.decode(output[0], skip_special_tokens=True).strip()
    return caption


if __name__ == "__main__":
    # A simple manual test that fetches a demo image and prints the caption.
    SAMPLE_IMAGE_URL = "https://huggingface.co/datasets/Narsil/image_dummy/resolve/main/parrots.png"

    print("Generating caption for sample image...")
    generated_caption = caption_image(SAMPLE_IMAGE_URL)
    print(f"Caption: {generated_caption}")
