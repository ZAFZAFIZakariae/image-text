"""Image captioning module using the BLIP large model.

This module provides a :func:`caption_image` function that loads either a local
image or one referenced by a URL and generates a caption using the
Salesforce BLIP image captioning model. The code follows the
recommendations from the BLIP model card on Hugging Face.
"""

from __future__ import annotations

from io import BytesIO
from typing import Dict, Tuple
from urllib.parse import urlparse

import requests
import torch
from PIL import Image
from transformers import BlipForConditionalGeneration, BlipProcessor

# Global cache of loaded processors/models keyed by model identifier so the
# weights are only materialized once per checkpoint. Loading the BLIP model is
# relatively expensive, so keeping the objects around avoids paying the cost for
# every call to ``caption_image``.
_LOADED_MODELS: Dict[
    str, Tuple[BlipProcessor, BlipForConditionalGeneration]
] = {}
_DEVICE: torch.device | None = None


def _is_url(path_or_url: str) -> bool:
    """Return ``True`` when ``path_or_url`` looks like an HTTP(S) URL."""

    parsed = urlparse(path_or_url)
    return parsed.scheme in {"http", "https"} and bool(parsed.netloc)


def _load_model(
    model_name: str = "Salesforce/blip-image-captioning-large",
) -> Tuple[BlipProcessor, BlipForConditionalGeneration, torch.device]:
    """Lazy-load the BLIP processor and model, returning both along with the device.

    The function caches the loaded objects in module-level globals to ensure that
    multiple caption requests reuse the same processor and model. The cache is
    keyed by ``model_name`` so callers can select different BLIP checkpoints.
    """

    global _DEVICE

    if _DEVICE is None:
        # Choose GPU when available, otherwise fall back to CPU execution.
        _DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    if model_name not in _LOADED_MODELS:
        # Load the pretrained processor and model requested by the caller. The
        # default remains the larger checkpoint for richer captions, but any
        # compatible BLIP variant can be supplied.
        processor = BlipProcessor.from_pretrained(model_name)
        model = BlipForConditionalGeneration.from_pretrained(model_name).to(_DEVICE)

        # Ensure the model is in evaluation mode. Caption generation does not
        # require gradient computation, so we disable it to save memory.
        model.eval()
        _LOADED_MODELS[model_name] = (processor, model)

    processor, model = _LOADED_MODELS[model_name]
    assert _DEVICE is not None
    return processor, model, _DEVICE


def caption_image(
    image_path_or_url: str,
    *,
    model_name: str = "Salesforce/blip-image-captioning-large",
) -> str:
    """Generate a descriptive caption for ``image_path_or_url``.

    Args:
        image_path_or_url: Either a filesystem path to an image or an HTTP(S)
            URL that returns image bytes.

    Returns:
        A caption generated by the BLIP model.

    Raises:
        requests.HTTPError: If downloading an image from a URL fails.
        PIL.UnidentifiedImageError: If the data cannot be interpreted as an image.
    """

    processor, model, device = _load_model(model_name)

    # Load the image from disk or over HTTP. We convert the image to RGB because
    # BLIP expects three channels.
    if _is_url(image_path_or_url):
        response = requests.get(image_path_or_url, timeout=30)
        response.raise_for_status()
        image = Image.open(BytesIO(response.content)).convert("RGB")
    else:
        image = Image.open(image_path_or_url).convert("RGB")

    # Run the image through the processor and the model to obtain caption tokens.
    inputs = processor(images=image, return_tensors="pt").to(device)

    with torch.no_grad():
        output = model.generate(**inputs)

    # Decode the generated tokens back into text and strip excess whitespace.
    caption = processor.decode(output[0], skip_special_tokens=True).strip()
    return caption


if __name__ == "__main__":
    # A simple manual test that fetches a demo image and prints the caption.
    SAMPLE_IMAGE_URL = "https://huggingface.co/datasets/Narsil/image_dummy/resolve/main/parrots.png"

    print("Generating caption for sample image...")
    generated_caption = caption_image(SAMPLE_IMAGE_URL)
    print(f"Caption: {generated_caption}")
